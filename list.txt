---- START OF ./app/__init__.py ----
from flask import Flask, render_template, request
import logging

# Set up logging configuration
logging.basicConfig(level=logging.INFO)

# Define the static folder explicitly
app = Flask(__name__, template_folder='../templates', static_folder='../static')

# Log when the Flask app starts
@app.before_first_request
def before_first_request():
    logging.info("Flask application has started")

# Log every request
@app.before_request
def log_request_info():
    logging.info(f"Request Method: {request.method}, Request Path: {request.path}")
    logging.info(f"Request Headers: {request.headers}")
    if request.method in ['POST', 'PUT', 'PATCH']:
        logging.info(f"Request Body: {request.get_data()}")

@app.route('/')
def index():
    logging.info("Rendering the index.html page")
    return render_template('index.html')

import os

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))  # Default to 8080 if PORT is not set
    app.run(host='0.0.0.0', port=port)

---- END OF ./app/__init__.py ----

---- START OF ./app/api.py ----
import os
from flask import Flask, request, jsonify, abort
import pandas as pd
import pickle
import logging
from sklearn.exceptions import NotFittedError
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set up logging
logging.basicConfig(level=logging.INFO)

app = Flask(__name__)

# Load the trained model
model_path = 'models/co2_model.pkl'

def load_model(model_path):
    """ Load the trained model from a file. """
    try:
        with open(model_path, 'rb') as file:
            model = pickle.load(file)
        logging.info(f"Model loaded successfully from {model_path}")
        return model
    except FileNotFoundError:
        logging.error(f"Model file not found: {model_path}")
        return None
    except Exception as e:
        logging.error(f"Error loading the model: {e}")
        return None

# Load the model
model = load_model(model_path)

# API endpoint to make predictions
@app.route('/api/predict', methods=['POST'])
def predict():
    try:
        if model is None:
            logging.error("Model is not available")
            abort(500, description='Model is not available')
        
        input_data = request.get_json()

        if not input_data:
            logging.error("No input data provided for prediction")
            abort(400, description='No input data provided')

        # Assuming your model expects a DataFrame with certain columns
        input_df = pd.DataFrame(input_data)
        logging.info(f"Received input data: {input_data}")
        
        if input_df.isnull().values.any():
            logging.error("Input data contains missing values")
            abort(400, description='Input data contains missing values')
        
        # Validate input format
        expected_columns = ['feature1', 'feature2']  # Replace with actual expected feature names
        if not all(col in input_df.columns for col in expected_columns):
            logging.error(f"Expected columns are missing. Expected: {expected_columns}, Received: {input_df.columns.tolist()}")
            abort(400, description=f"Expected columns: {expected_columns}")

        try:
            predictions = model.predict(input_df)
            logging.info("Prediction successful")
        except NotFittedError:
            logging.error("Model is not fitted correctly")
            abort(500, description="Model is not fitted correctly.")
        
        return jsonify({'predictions': predictions.tolist()})
    
    except Exception as e:
        logging.error(f"An error occurred during prediction: {str(e)}")
        return jsonify({'error': f"An error occurred: {str(e)}"}), 500

# API endpoint for health check
@app.route('/api/health', methods=['GET'])
def health_check():
    logging.info("Health check endpoint accessed")
    return jsonify({'status': 'API is running'}), 200

from flask import Flask, render_template, request
import logging

# Set up logging configuration
logging.basicConfig(level=logging.INFO)

# Define the static folder explicitly
app = Flask(__name__, template_folder='../templates', static_folder='../static')

# Log when the Flask app starts
@app.before_first_request
def before_first_request():
    logging.info("Flask application has started")

# Log every request
@app.before_request
def log_request_info():
    logging.info(f"Request Method: {request.method}, Request Path: {request.path}")
    logging.info(f"Request Headers: {request.headers}")
    if request.method in ['POST', 'PUT', 'PATCH']:
        logging.info(f"Request Body: {request.get_data()}")

@app.route('/')
def index():
    logging.info("Rendering the index.html page")
    return render_template('index.html')

import os

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))  # Default to 8080 if PORT is not set
    app.run(host='0.0.0.0', port=port)


---- END OF ./app/api.py ----

---- START OF ./app/routes.py ----
from flask import Flask, render_template, jsonify, request, abort
import pandas as pd
import logging
from sqlalchemy import create_engine
import plotly.express as px
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set up logging
logging.basicConfig(level=logging.INFO)


app = Flask(__name__)

# Load database URL from environment variables
DATABASE_URL = os.getenv("DATABASE_URL")

if not DATABASE_URL:
    logging.error("No DATABASE_URL set for PostgreSQL connection")
    raise ValueError("No DATABASE_URL set for PostgreSQL connection")

# Connect to PostgreSQL using SQLAlchemy
try:
    engine = create_engine(DATABASE_URL)
    logging.info("Connected to PostgreSQL successfully")
except Exception as e:
    logging.error(f"Error connecting to PostgreSQL: {e}")
    raise

@app.route('/')
def index():
    try:
        logging.info("Loading data from the database")
        # Load data from the database
        df = pd.read_sql("SELECT * FROM emissions_data", engine)
        logging.info(f"Data loaded: {df.shape} rows, columns: {df.columns.tolist()}")
        
        # Check if the dataframe is empty
        if df.empty:
            logging.warning("No data returned from the database.")
            return render_template('index.html')

        logging.info("Data loaded successfully from the database")

        # Clean up column names
        df.columns = df.columns.str.strip().str.replace(' ', '_')

        # Visualization 1: Global CO2 Emissions Over Time
        years = df.columns[5:]
        global_emissions = df[years].sum()
        logging.info(f"Global emissions data: {global_emissions}")  # Log global emissions
        global_emissions_fig = px.line(
            x=years,
            y=global_emissions,
            labels={'x': 'Year', 'y': 'Global CO2 Emissions (kt)'},
            title='Global CO2 Emissions Over Time'
        )
        logging.info("Global CO2 Emissions visualization created")
        logging.info(global_emissions_fig.to_json())  # Log the figure JSON

        # Visualization 2: CO2 Emissions by Country (2020)
        df_2020 = df[['country_name', '2020']]
        logging.info(f"2020 emissions data: {df_2020.head()}")  # Log a sample of the data
        country_emissions_fig = px.choropleth(
            df_2020,
            locations='country_name',
            locationmode='country names',
            color='2020',
            hover_name='country_name',
            title='CO2 Emissions by Country (2020)',
            color_continuous_scale=px.colors.sequential.Plasma
        )
        logging.info("CO2 Emissions by Country visualization created")
        logging.info(country_emissions_fig.to_json())  # Log the figure JSON

        # Visualization 3: Top 10 CO2 Emitting Countries
        top_emitters = df[['country_name', '2020']].sort_values(by='2020', ascending=False).head(10)
        logging.info(f"Top 10 CO2 emitters: {top_emitters}")  # Log top 10 emitters
        top_emitters_fig = px.bar(
            top_emitters,
            x='country_name',
            y='2020',
            title='Top 10 CO2 Emitting Countries (2020)',
            labels={'2020': 'CO2 Emissions (kt)', 'country_name': 'Country'}
        )
        logging.info("Top 10 CO2 Emitting Countries visualization created")
        logging.info(top_emitters_fig.to_json())  # Log the figure JSON

        # Visualization 4: CO2 Emissions Over Time for Selected Countries
        selected_countries = ['United States', 'China', 'India', 'Russia', 'Germany']
        df_selected = df[df['country_name'].isin(selected_countries)]
        logging.info(f"Data for selected countries: {df_selected.head()}")  # Log a sample
        df_selected = df_selected.melt(id_vars=['country_name'], value_vars=years, var_name='Year', value_name='CO2 Emissions (kt)')
        selected_countries_fig = px.line(
            df_selected,
            x='Year',
            y='CO2 Emissions (kt)',
            color='country_name',
            title='CO2 Emissions Over Time for Selected Countries'
        )
        logging.info("CO2 Emissions Over Time for Selected Countries visualization created")
        logging.info(selected_countries_fig.to_json())  # Log the figure JSON

        # Visualization 5: CO2 Emissions vs Population (if population data is available)
        if 'population' in df.columns:
            logging.info(f"Population data exists, creating scatter plot.")
            emissions_vs_population_fig = px.scatter(
                df,
                x='population',
                y='2020',
                size='2020',
                hover_name='country_name',
                title='CO2 Emissions vs Population (2020)',
                labels={'2020': 'CO2 Emissions (kt)', 'population': 'Population'}
            )
            logging.info("CO2 Emissions vs Population visualization created")
            logging.info(emissions_vs_population_fig.to_json())  # Log the figure JSON
        else:
            emissions_vs_population_fig = None
            logging.warning("Population data not available for CO2 Emissions vs Population visualization")

        # Render all visualizations on the main page
        logging.info("Rendering visualizations on index.html")
        return render_template(
            'index.html',
            global_emissions_data=global_emissions_fig.to_json(),
            country_emissions_data=country_emissions_fig.to_json(),
            top_emitters_data=top_emitters_fig.to_json(),
            selected_countries_data=selected_countries_fig.to_json(),
            emissions_vs_population_data=emissions_vs_population_fig.to_json() if emissions_vs_population_fig else ''
        )

    except Exception as e:
        logging.error(f"An error occurred during index processing: {str(e)}")
        return jsonify({'error': f"An error occurred: {str(e)}"}), 500

---- END OF ./app/routes.py ----

---- START OF ./requirements.txt ----
Flask==2.0.1
pandas==1.3.3
scikit-learn==0.24.2
plotly==5.3.1
numpy==1.21.2
gunicorn==20.1.0
psycopg2-binary==2.9.1
python-dotenv==0.19.2
SQLAlchemy==1.4.22
---- END OF ./requirements.txt ----

---- START OF ./config/settings.json ----
{
    "database": {
        "type": "postgresql",
        "url": ""
    },
    "api": {
        "prediction_endpoint": "/api/predict",
        "health_endpoint": "/api/health"
    },
    "api_keys": {
        "service_1": "",
        "service_2": ""
    }
}---- END OF ./config/settings.json ----

---- START OF ./config/runtime.txt ----

python-3.8.12
---- END OF ./config/runtime.txt ----

---- START OF ./tests/test_model.py ----

import unittest
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import pickle

class TestModelTraining(unittest.TestCase):
    
    def setUp(self):
        # Example training data
        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
        y = np.array([2, 3, 4, 5, 6])
        
        # Split the data into training and test sets
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train a simple Linear Regression model
        self.model = LinearRegression()
        self.model.fit(self.X_train, self.y_train)
        
        # Save the model using pickle (mocking real use case)
        self.model_file = 'test_model.pkl'
        with open(self.model_file, 'wb') as file:
            pickle.dump(self.model, file)
    
    def test_model_training(self):
        # Check if the model was trained successfully
        y_pred_train = self.model.predict(self.X_train)
        self.assertEqual(len(y_pred_train), len(self.y_train), "Model training failed")
    
    def test_model_prediction(self):
        # Test model prediction accuracy on the test set
        y_pred_test = self.model.predict(self.X_test)
        mse = mean_squared_error(self.y_test, y_pred_test)
        self.assertLess(mse, 0.1, "Model test predictions are not accurate enough")
    
    def tearDown(self):
        # Clean up the saved model file
        import os
        if os.path.exists(self.model_file):
            os.remove(self.model_file)

if __name__ == '__main__':
    unittest.main()
---- END OF ./tests/test_model.py ----

---- START OF ./tests/test_data_cleaning.py ----

import unittest
import pandas as pd
from scripts.data_cleaning import clean_data

class TestDataCleaning(unittest.TestCase):
    
    def setUp(self):
        # Example raw data for testing
        data = {
            'Country Name': ['Country A', 'Country B', 'Country C'],
            'Year 1': [10, 20, None],
            'Year 2': [15, None, 30],
            'Year 3': [None, 25, 35]
        }
        self.df = pd.DataFrame(data)
    
    def test_clean_data(self):
        # Run the data cleaning function
        df_cleaned = clean_data(self.df)
        
        # Check if missing values are filled
        self.assertFalse(df_cleaned.isnull().values.any(), "There are still missing values in the cleaned data")
        
        # Check if the shape of the DataFrame remains unchanged
        self.assertEqual(df_cleaned.shape, self.df.shape, "DataFrame shape changed after cleaning")
    
    def tearDown(self):
        # Clean up (if necessary)
        pass

if __name__ == '__main__':
    unittest.main()
---- END OF ./tests/test_data_cleaning.py ----

---- START OF ./tests/test_api.py ----
import unittest
from unittest.mock import patch
import json
from app.api import app

class TestAPI(unittest.TestCase):
    
    def setUp(self):
        self.app = app.test_client()
        self.app.testing = True

    def test_health_check(self):
        response = self.app.get('/api/health')
        self.assertEqual(response.status_code, 200)
        data = json.loads(response.data)
        self.assertEqual(data['status'], 'API is running')

    @patch('app.api.model.predict')
    def test_prediction(self, mock_predict):
        mock_input = {
            "feature1": [6, 7, 8],
            "feature2": [7, 8, 9]
        }
        mock_predict.return_value = [1, 2, 3]  # Mocking the prediction

        response = self.app.post('/api/predict', data=json.dumps(mock_input), content_type='application/json')
        self.assertEqual(response.status_code, 200)
        data = json.loads(response.data)
        self.assertIn('predictions', data)
        self.assertEqual(data['predictions'], [1, 2, 3])

if __name__ == '__main__':
    unittest.main()
---- END OF ./tests/test_api.py ----

---- START OF ./models/co2_best_model.pkl ----
Äïx      åsklearn.linear_model._baseîåLinearRegressionîìî)Åî}î(åfit_interceptîàåcopy_Xîàån_jobsîNåpositiveîâån_features_in_îKåcoef_îånumpy.core.multiarrayîå_reconstructîìîånumpyîåndarrayîìîK ÖîCbîáîRî(KKÖîhådtypeîìîåf8îâàáîRî(Kå<îNNNJˇˇˇˇJˇˇˇˇK tîbâC¯Xé∆€Õ†œ<    ÄbP=T5…äÎî|=?
d˚áΩTHmQ¯Ä=ÖØq&9vΩ2ﬁ∑;gY`Ω*¨¸≠Rg[=xÇÕx˙t=bø»n‹ﬁt=ºó€‡tΩ⁄™ ÏZbèΩ.ÿ}ﬁ	åÄ=¯üÅÒëi= ,:»l?=n)§§X©a=åg'ˆD=H±^‹˛‚`=ÂÚÂ®¨ÄΩN#]OÊa=ıñçe≈s=\ö}xdÈrΩ]·	^∏hΩ Â”q cgΩê≤à,]Ñ=ÍπFí¨„qΩ¬\Ön–m=¥∫‰˙z¸>ΩKõ4ß=jcΩ
è£9Lu_Ω»	    ?îtîbårank_îKå	singular_îhhK ÖîháîRî(KKÖîhâC¯Œ˘>m∞AS»Ór6ÒÉA~RÊièﬁ]A?ı4±MA˙ )±Í@AÜ’‚üΩÇ%A Ì'ÌE
Aöjh´Aπ\6˘,AîûM	A$è†‹Ú<A°Û∂l1ÂAÆ7ﬂ	+W A»lΩ•¯ı˝@•	ûŸ@º˜@b%z6(÷Ù@≤~Y"—cÛ@˜ƒìÀ⁄ÛÒ@™Û®¥Ï@ésâØ3ùÍ@U¸è^º≥‰@™∆¥=lR·@IX'O-–ﬂ@u±õΩ‹@5·≤±h‹@^O^.JuŸ@∞€¥ﬁk(÷@–úì	©“@g˜,g“@Å¬Ω'#òÃ@πö¢É5Â∆@îtîbå
intercept_îhåscalarîìîhC      AæîÜîRîå_sklearn_versionîå1.4.2îub.---- END OF ./models/co2_best_model.pkl ----

---- START OF ./models/co2_model.pkl ----
Äï®      åsklearn.linear_model._baseîåLinearRegressionîìî)Åî}î(åfit_interceptîàåcopy_Xîàån_jobsîNåpositiveîâåfeature_names_in_îånumpy.core.multiarrayîå_reconstructîìîånumpyîåndarrayîìîK ÖîCbîáîRî(KKÖîhådtypeîìîåO8îâàáîRî(Kå|îNNNJˇˇˇˇJˇˇˇˇK?tîbâ]î(å1990îå1991îå1992îå1993îå1994îå1995îå1996îå1997îå1998îå1999îå2000îå2001îå2002îå2003îå2004îå2005îå2006îå2007îå2008îå2009îå2010îå2011îå2012îå2013îå2014îå2015îå2016îå2017îå2018îå2019îå2020îetîbån_features_in_îKåcoef_îhhK ÖîháîRî(KKÖîhåf8îâàáîRî(Kå<îNNNJˇˇˇˇJˇˇˇˇK tîbâC¯à˚U0‡>ï≤∫`A„SøkÅUÊ£i=?DbÉŸOq?OÀWûb?Hk-YÜ$øUh|œ>‘G?}Â°…Yøÿ€§∆”W?»Á+•´#ø¨öÕªü;? ñ`ærrøÓVaÓr?CÌYG|øî˛èåïhk?`Fr◊‰a?†£|F…\?JÒˆ·ªMøË´
é>{ø1+9—† eø◊≥≈¯\+F?L5Ã& \ø§£Ë`=$p?dìı,+vø∑)∆uß{?èûœcaxzøêÂƒ[ u?íπÑ™73bø) åÄ˛Qº>√lﬁC√m4?⁄+|y°øîtîbårank_îKå	singular_îhhK ÖîháîRî(KKÖîhEâC¯K†?m∞AÆsÄr6ÒÉAÿª´ièﬁ]A§Õ84±MAìœs≥Í@AıËºüΩÇ%AxIÎE
Aè∏ÎZ´Ay˛±÷¯,AÂ¿(ûM	A∂^’Ú<AìÉ¶i1ÂA¥vT6+W ARè	Ä¯ı˝@¸ ·Ë@º˜@wÜ€D(÷Ù@î¸—cÛ@€ÛÑ1€ÛÒ@˘Â
›ß¥Ï@:∞p¡3ùÍ@«á º≥‰@pÓ≤kR·@x∏=-–ﬂ@ﬁ.éôΩ‹@÷…÷i≥h‹@ﬂI1ôJuŸ@RÃjp(÷@—OìÎ©“@ËÊe⁄h“@aÍ"òÃ@v∫™¬6Â∆@îtîbå
intercept_îh
åscalarîìîhECÁ◊YEaº`@îÜîRîå_sklearn_versionîå1.4.2îub.---- END OF ./models/co2_model.pkl ----

---- START OF ./.elasticbeanstalk/config.yml ----
branch-defaults:
  main:
    environment: CO2-env
environment-defaults:
  CO2-env:
    branch: null
    repository: null
global:
  application_name: CO2
  default_ec2_keyname: aws-eb
  default_platform: Python 3.8 running on 64bit Amazon Linux 2
  default_region: us-east-1
  include_git_submodules: true
  instance_profile: null
  platform_name: null
  platform_version: null
  profile: eb-cli
  sc: git
  workspace_type: Application
---- END OF ./.elasticbeanstalk/config.yml ----

---- START OF ./.env ----
DATABASE_URL=postgresql://postgres:co2JD613947#1@co2emissions.cluster-clpfmbvmfsji.us-east-1.rds.amazonaws.com:5432/co2emissions
SERVICE_1_API_KEY=your_service_1_api_key
SERVICE_2_API_KEY=your_service_2_api_key


---- END OF ./.env ----

---- START OF ./static/css/style.css ----
body {
    font-family: Arial, sans-serif;
    background-color: #f4f4f9;
    color: #333;
    margin: 0;
    padding: 0;
}

header {
    background-color: #4CAF50;
    color: white;
    padding: 10px 0;
    text-align: center;
}

h1,
h2 {
    color: #333;
}

.container {
    width: 80%;
    margin: 0 auto;
}

button {
    background-color: #4CAF50;
    color: white;
    padding: 10px 20px;
    border: none;
    cursor: pointer;
    border-radius: 4px;
    margin: 10px 0;
}

button:hover {
    background-color: #45a049;
}

footer {
    text-align: center;
    padding: 10px;
    background-color: #4CAF50;
    color: white;
    position: fixed;
    bottom: 0;
    width: 100%;
}

.chart-container {
    width: 100%;
    height: 400px;
    margin-top: 20px;
}---- END OF ./static/css/style.css ----

---- START OF ./static/js/dashboard.js ----
// JavaScript for interactive visualizations using Plotly

document.addEventListener('DOMContentLoaded', function () {
    // Example data for CO2 emissions over time
    var years = ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020'];
    var emissions = [10000, 12000, 15000, 14000, 13000, 12500, 13500, 14500, 16000, 17000, 18000];  // Example values

    var trace1 = {
        x: years,
        y: emissions,
        type: 'scatter',
        mode: 'lines+markers',
        name: 'CO2 Emissions',
        line: { shape: 'linear', color: '#1f77b4' },
        marker: { size: 8 }
    };

    var data = [trace1];

    var layout = {
        title: 'CO2 Emissions Over Time',
        xaxis: { title: 'Year' },
        yaxis: { title: 'CO2 Emissions (kt)' }
    };

    // Render the chart in a div with id 'co2-chart'
    Plotly.newPlot('co2-chart', data, layout);
});
---- END OF ./static/js/dashboard.js ----

---- START OF ./scripts/train_model.py ----
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import pickle

def load_data_from_db(engine):
    """Load data from AWS RDS."""
    query = "SELECT * FROM emissions_data"
    df = pd.read_sql(query, engine)
    print("Data loaded successfully from the database.")
    return df

def preprocess_data(X, y):
    """Impute missing values in the feature matrix and target."""
    imputer = SimpleImputer(strategy='mean')  # You can also try 'median', 'most_frequent', etc.
    
    # Impute X (features)
    X_imputed = imputer.fit_transform(X)
    
    # Drop rows where y (target) is NaN
    non_nan_indices = y.notna()
    X_imputed = X_imputed[non_nan_indices]
    y = y[non_nan_indices]
    
    return X_imputed, y

def handle_missing_columns(df):
    """Drop columns with all missing values."""
    df = df.dropna(axis=1, how='all')
    return df

def train_model(X_train, y_train):
    """Train a model using GridSearchCV for hyperparameter tuning."""
    # Define models to test
    models = {
        'linear_regression': LinearRegression(),
        'random_forest': RandomForestRegressor(),
        'hist_gradient_boosting': HistGradientBoostingRegressor()  # Handles missing values natively
    }
    
    # Define parameters for tuning
    param_grid = {
        'linear_regression': {},
        'random_forest': {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, None]
        },
        'hist_gradient_boosting': {
            'max_iter': [100, 200],
            'max_depth': [10, 20, None]
        }
    }

    best_model = None
    best_score = float('inf')
    
    for model_name in models:
        print(f"Training {model_name}...")
        grid = GridSearchCV(models[model_name], param_grid[model_name], cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
        grid.fit(X_train, y_train)
        
        print(f"Best parameters for {model_name}: {grid.best_params_}")
        print(f"Best score for {model_name}: {-grid.best_score_}")
        
        if -grid.best_score_ < best_score:
            best_score = -grid.best_score_
            best_model = grid.best_estimator_

    print("Best model selected.")
    return best_model

def evaluate_model(model, X_test, y_test):
    """Evaluate the model's performance."""
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Mean Squared Error: {mse}")
    print(f"R^2 Score: {r2}")
    return mse, r2

def save_model(model, output_file_path):
    """Save the trained model to a file using pickle."""
    with open(output_file_path, 'wb') as file:
        pickle.dump(model, file)
    print(f"Model saved to {output_file_path}")

if __name__ == "__main__":
    from sqlalchemy import create_engine
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()

    # Database connection
    DATABASE_URL = os.getenv("DATABASE_URL")
    engine = create_engine(DATABASE_URL)

    # Load the cleaned data from the database
    df = load_data_from_db(engine)

    if df is not None:
        # Drop columns with all missing values
        df = handle_missing_columns(df)

        # Define features (X) and target (y)
        X = df.iloc[:, 5:]  # Assuming numerical features start from the 5th column onward
        y = df['2020']  # Adjust target column to the year of interest (e.g., '2020')

        # Preprocess the data to handle NaNs in both X and y
        X_imputed, y_cleaned = preprocess_data(X, y)

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_cleaned, test_size=0.2, random_state=42)

        # Train the model with hyperparameter tuning
        model = train_model(X_train, y_train)

        # Evaluate the model
        evaluate_model(model, X_test, y_test)

        # Save the best model
        model_output_path = 'models/co2_best_model.pkl'
        save_model(model, model_output_path)
---- END OF ./scripts/train_model.py ----

---- START OF ./scripts/predict.py ----

import pickle
import pandas as pd

def load_model(model_file_path):
    """ Load the trained model from a file. """
    try:
        with open(model_file_path, 'rb') as file:
            model = pickle.load(file)
        print(f"Model loaded successfully from {model_file_path}")
        return model
    except FileNotFoundError as e:
        print(f"Model file not found: {e}")
        return None

def make_prediction(model, input_data):
    """ Make predictions using the loaded model and input data. """
    predictions = model.predict(input_data)
    return predictions

if __name__ == "__main__":
    # Path to the saved model
    model_file_path = 'models/co2_model.pkl'  # Replace with the actual model file path

    # Load the model
    model = load_model(model_file_path)

    if model is not None:
        # Example input data (replace with actual data or dynamically load it)
        input_data = pd.DataFrame({
            'feature1': [6, 7, 8],  # Example feature columns
            'feature2': [7, 8, 9]
        })

        # Make predictions
        predictions = make_prediction(model, input_data)

        # Display the predictions
        print("Predictions for the input data:")
        print(predictions)
---- END OF ./scripts/predict.py ----

---- START OF ./scripts/database_setup.py ----
import os
import psycopg2
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

DATABASE_URL = os.getenv("DATABASE_URL")

def create_database_connection():
    """ Connect to PostgreSQL and create a database connection. """
    try:
        conn = psycopg2.connect(DATABASE_URL)
        print("Connection to PostgreSQL DB successful")
        return conn
    except Exception as e:
        print(f"Error: {e}")
        return None

def create_table(conn):
    """ Drop and create a table for storing CO2 emissions data. """
    drop_table_sql = '''
    DROP TABLE IF EXISTS emissions_data;
    '''
    
    create_table_sql = '''
    CREATE TABLE emissions_data (
        id SERIAL PRIMARY KEY,
        country_name TEXT NOT NULL,
        country_code TEXT,
        indicator_name TEXT,
        indicator_code TEXT,
        "1960" REAL,
        "1961" REAL,
        "1962" REAL,
        "1963" REAL,
        "1964" REAL,
        "1965" REAL,
        "1966" REAL,
        "1967" REAL,
        "1968" REAL,
        "1969" REAL,
        "1970" REAL,
        "1971" REAL,
        "1972" REAL,
        "1973" REAL,
        "1974" REAL,
        "1975" REAL,
        "1976" REAL,
        "1977" REAL,
        "1978" REAL,
        "1979" REAL,
        "1980" REAL,
        "1981" REAL,
        "1982" REAL,
        "1983" REAL,
        "1984" REAL,
        "1985" REAL,
        "1986" REAL,
        "1987" REAL,
        "1988" REAL,
        "1989" REAL,
        "1990" REAL,
        "1991" REAL,
        "1992" REAL,
        "1993" REAL,
        "1994" REAL,
        "1995" REAL,
        "1996" REAL,
        "1997" REAL,
        "1998" REAL,
        "1999" REAL,
        "2000" REAL,
        "2001" REAL,
        "2002" REAL,
        "2003" REAL,
        "2004" REAL,
        "2005" REAL,
        "2006" REAL,
        "2007" REAL,
        "2008" REAL,
        "2009" REAL,
        "2010" REAL,
        "2011" REAL,
        "2012" REAL,
        "2013" REAL,
        "2014" REAL,
        "2015" REAL,
        "2016" REAL,
        "2017" REAL,
        "2018" REAL,
        "2019" REAL,
        "2020" REAL,
        "2021" REAL,
        "2022" REAL,
        "2023" REAL
    );
    '''


    try:
        with conn.cursor() as cursor:
            cursor.execute(drop_table_sql)  # Drop the table if it exists
            cursor.execute(create_table_sql)  # Create the new table
            conn.commit()
            print("Table 'emissions_data' created successfully.")
    except Exception as e:
        print(f"Error creating table: {e}")

def close_connection(conn):
    """ Close the connection to the PostgreSQL database. """
    conn.close()
    print("Database connection closed.")

if __name__ == "__main__":
    conn = create_database_connection()
    if conn:
        create_table(conn)
        close_connection(conn)
---- END OF ./scripts/database_setup.py ----

---- START OF ./scripts/data_cleaning.py ----
import pandas as pd

def load_data(file_path):
    """Load the CO2 emissions dataset."""
    try:
        df = pd.read_csv(file_path, skiprows=4)  # Adjust for file structure (skip metadata rows)
        print(f"Data loaded successfully from {file_path}")
        return df
    except FileNotFoundError as e:
        print(f"File not found: {e}")
        return None

def clean_data(df):
    """Clean and preprocess the CO2 emissions data."""
    # Remove unnamed columns and fill missing values
    df_cleaned = df.loc[:, ~df.columns.str.contains('^Unnamed')].copy()

    # Forward-fill missing values
    df_cleaned.ffill(inplace=True)
    
    # Ensure no rows with NaN remain
    df_cleaned.dropna(how='all', inplace=True)

    # Rename columns to match PostgreSQL table schema (remove spaces, lowercase, use underscores)
    df_cleaned.columns = df_cleaned.columns.str.strip().str.lower().str.replace(' ', '_')
    
    print("Data cleaning and renaming columns completed.")
    return df_cleaned

def save_cleaned_data(df, output_file_path):
    """Save the cleaned data to a new CSV file."""
    df.to_csv(output_file_path, index=False)
    print(f"Cleaned data saved to {output_file_path}")

if __name__ == "__main__":
    # Example: File paths
    input_file_path = 'data/raw/API_EN.ATM.CO2E.KT_DS2_en_csv_v2_32234.csv'  # Replace with your actual path
    output_file_path = 'data/cleaned/co2_emissions_cleaned.csv'  # Replace with your actual path
    
    # Load and clean data
    df = load_data(input_file_path)
    if df is not None:
        df_cleaned = clean_data(df)
        save_cleaned_data(df_cleaned, output_file_path)
---- END OF ./scripts/data_cleaning.py ----

---- START OF ./scripts/upload_cleaned_data.py ----
import os
import pandas as pd
import psycopg2
from sqlalchemy import create_engine
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Get database URL from environment variables
DATABASE_URL = os.getenv("DATABASE_URL")

def load_cleaned_data(file_path):
    """Load cleaned CO2 emissions data from CSV."""
    try:
        df = pd.read_csv(file_path)
        print(f"Data loaded successfully from {file_path}")
        return df
    except FileNotFoundError as e:
        print(f"File not found: {e}")
        return None

def upload_to_rds(df, table_name, db_url):
    """Upload DataFrame to AWS RDS PostgreSQL table."""
    try:
        engine = create_engine(db_url)
        with engine.connect() as connection:
            df.to_sql(table_name, connection, if_exists='append', index=False)
            print(f"Data uploaded successfully to {table_name} table.")
    except Exception as e:
        print(f"Error uploading data: {e}")

if __name__ == "__main__":
    # Path to the cleaned data
    cleaned_data_path = 'data/cleaned/co2_emissions_cleaned.csv'  # Adjust this path if necessary

    # Load the cleaned data
    df_cleaned = load_cleaned_data(cleaned_data_path)

    # Upload the data to RDS if loaded successfully
    if df_cleaned is not None:
        upload_to_rds(df_cleaned, "emissions_data", DATABASE_URL)
---- END OF ./scripts/upload_cleaned_data.py ----

---- START OF ./.github/workflows/deploy.yml ----
name: Deploy to Elastic Beanstalk

on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8'  # Your project's Python version

      - name: Install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install -r requirements.txt

      - name: Deploy to Elastic Beanstalk
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          pip install awsebcli
          eb init -p python-3.x -r ${{ secrets.AWS_REGION }} ${{ secrets.APPLICATION_NAME }} --interactive
          eb deploy ${{ secrets.ENV_NAME }}
---- END OF ./.github/workflows/deploy.yml ----

---- START OF ./templates/index.html ----
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CO2 Emission Dashboard</title>
    <link rel="stylesheet" href="/static/css/style.css">
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</head>

<body>
    <header>
        <h1>CO2 Emission Dashboard</h1>
    </header>

    <div class="container">
        <h2>Global CO2 Emissions Over Time</h2>
        <div id="global_emissions_chart" class="chart-container"></div>

        <h2>CO2 Emissions by Country (2020)</h2>
        <div id="country_emissions_chart" class="chart-container"></div>

        <h2>Top 10 CO2 Emitting Countries (2020)</h2>
        <div id="top_emitters_chart" class="chart-container"></div>

        <h2>CO2 Emissions Over Time for Selected Countries</h2>
        <div id="selected_countries_chart" class="chart-container"></div>

        <h2>CO2 Emissions vs Population (2020)</h2>
        <div id="emissions_vs_population_chart" class="chart-container"></div>
    </div>

    <footer>
        <p>&copy; 2024 CO2 Emission Dashboard</p>
    </footer>

    <script type="text/javascript">
        {% if global_emissions_data %}
        var globalEmissions = {{ global_emissions_data | tojson | safe }};
        Plotly.newPlot('global_emissions_chart', globalEmissions.data, globalEmissions.layout);
        {% else %}
        document.getElementById('global_emissions_chart').innerHTML = "<p>No data available for Global CO2 Emissions Over Time</p>";
        {% endif %}

        {% if country_emissions_data %}
        var countryEmissions = {{ country_emissions_data | tojson | safe }};
        Plotly.newPlot('country_emissions_chart', countryEmissions.data, countryEmissions.layout);
        {% else %}
        document.getElementById('country_emissions_chart').innerHTML = "<p>No data available for CO2 Emissions by Country (2020)</p>";
        {% endif %}

        {% if top_emitters_data %}
        var topEmitters = {{ top_emitters_data | tojson | safe }};
        Plotly.newPlot('top_emitters_chart', topEmitters.data, topEmitters.layout);
        {% else %}
        document.getElementById('top_emitters_chart').innerHTML = "<p>No data available for Top 10 CO2 Emitting Countries (2020)</p>";
        {% endif %}

        {% if selected_countries_data %}
        var selectedCountries = {{ selected_countries_data | tojson | safe }};
        Plotly.newPlot('selected_countries_chart', selectedCountries.data, selectedCountries.layout);
        {% else %}
        document.getElementById('selected_countries_chart').innerHTML = "<p>No data available for CO2 Emissions Over Time for Selected Countries</p>";
        {% endif %}

        {% if emissions_vs_population_data %}
        var emissionsVsPopulation = {{ emissions_vs_population_data | tojson | safe }};
        Plotly.newPlot('emissions_vs_population_chart', emissionsVsPopulation.data, emissionsVsPopulation.layout);
        {% else %}
        document.getElementById('emissions_vs_population_chart').innerHTML = "<p>No data available for CO2 Emissions vs Population (2020)</p>";
        {% endif %}
    </script>

</body>

</html>---- END OF ./templates/index.html ----

---- START OF ./templates/chart.html ----
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CO2 Emission Visualization</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</head>

<body>
    <div id="chart"></div>
    <script type="text/javascript">
        var graphJSON = {{ graphJSON | safe }};
        Plotly.newPlot('chart', graphJSON.data, graphJSON.layout);
    </script>
</body>

</html>---- END OF ./templates/chart.html ----

---- START OF ./Procfile ----
web: gunicorn app.__init__:app --bind 0.0.0.0:$PORT
---- END OF ./Procfile ----

---- START OF ./data/database/emissions_data.sql ----
CREATE TABLE
    emissions_data (
        id SERIAL PRIMARY KEY,
        country_name VARCHAR(255),
        country_code VARCHAR(10),
        indicator_name VARCHAR(255),
        indicator_code VARCHAR(50),
        year_1960 FLOAT,
        year_1961 FLOAT,
        year_1962 FLOAT,
        year_1963 FLOAT,
        year_1964 FLOAT,
        year_1965 FLOAT,
        year_1966 FLOAT,
        year_1967 FLOAT,
        year_1968 FLOAT,
        year_1969 FLOAT,
        year_1970 FLOAT,
        year_1971 FLOAT,
        year_1972 FLOAT,
        year_1973 FLOAT,
        year_1974 FLOAT,
        year_1975 FLOAT,
        year_1976 FLOAT,
        year_1977 FLOAT,
        year_1978 FLOAT,
        year_1979 FLOAT,
        year_1980 FLOAT,
        year_1981 FLOAT,
        year_1982 FLOAT,
        year_1983 FLOAT,
        year_1984 FLOAT,
        year_1985 FLOAT,
        year_1986 FLOAT,
        year_1987 FLOAT,
        year_1988 FLOAT,
        year_1989 FLOAT,
        year_1990 FLOAT,
        year_1991 FLOAT,
        year_1992 FLOAT,
        year_1993 FLOAT,
        year_1994 FLOAT,
        year_1995 FLOAT,
        year_1996 FLOAT,
        year_1997 FLOAT,
        year_1998 FLOAT,
        year_1999 FLOAT,
        year_2000 FLOAT,
        year_2001 FLOAT,
        year_2002 FLOAT,
        year_2003 FLOAT,
        year_2004 FLOAT,
        year_2005 FLOAT,
        year_2006 FLOAT,
        year_2007 FLOAT,
        year_2008 FLOAT,
        year_2009 FLOAT,
        year_2010 FLOAT,
        year_2011 FLOAT,
        year_2012 FLOAT,
        year_2013 FLOAT,
        year_2014 FLOAT,
        year_2015 FLOAT,
        year_2016 FLOAT,
        year_2017 FLOAT,
        year_2018 FLOAT,
        year_2019 FLOAT,
        year_2020 FLOAT,
        year_2021 FLOAT,
        year_2022 FLOAT,
        year_2023 FLOAT
    );---- END OF ./data/database/emissions_data.sql ----

---- START OF ./notebooks/01_data_exploration.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# CO2 Emission Data Exploration\n","In this notebook, we will explore the CO2 emissions dataset to get a better understanding of its structure, missing values, and general trends. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the dataset\n","data_path = 'data/raw/co2_emissions.csv'\n","df = pd.read_csv(data_path, skiprows=4)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Check for Missing Values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check for missing values\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["## Summary Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display summary statistics of the dataset\n","df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualize Trends Over Time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example visualization of CO2 emissions for a country over time\n","df_country = df[df['Country Name'] == 'United States']\n","years = df.columns[4:-1]\n","emissions = df_country[years].T\n","plt.figure(figsize=(10,6))\n","plt.plot(years, emissions)\n","plt.title('CO2 Emissions Over Time (United States)')\n","plt.xlabel('Year')\n","plt.ylabel('CO2 Emissions (kt)')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Further Exploration Ideas\n","- Investigate emissions for other countries.\n","- Analyze the countries with the highest and lowest emissions.\n","- Check if there's a trend in global emissions over time."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/01_data_exploration.ipynb ----

---- START OF ./notebooks/05_deployment.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CO2 Emission Model Deployment and Performance Testing\n","In this notebook, we will save the trained machine learning model and test its performance in a deployment-like environment."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pickle\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.linear_model import LinearRegression  # Example ML model"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Preprocessed Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the preprocessed data\n","data_path = 'data/cleaned/co2_emissions_cleaned.csv'\n","df = pd.read_csv(data_path)\n","X = df.iloc[:, 2:]  # Feature columns\n","y = df.iloc[:, 1]   # Target column (CO2 emissions for a specific year)\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train and Save the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train a Linear Regression model (as an example)\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Save the trained model using pickle\n","model_filename = 'models/co2_model.pkl'\n","with open(model_filename, 'wb') as file:\n","    pickle.dump(model, file)\n","print(f'Model saved as {model_filename}')"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Model and Test Performance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the saved model\n","with open(model_filename, 'rb') as file:\n","    loaded_model = pickle.load(file)\n","print('Model loaded successfully')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate Model Performance on Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions on the test set\n","y_pred = loaded_model.predict(X_test)\n","\n","# Evaluate performance\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","print(f'Mean Squared Error: {mse}')\n","print(f'R^2 Score: {r2}')"]},{"cell_type":"markdown","metadata":{},"source":["## Next Steps\n","- Deploy the model to a cloud environment (e.g., AWS, Google Cloud) if needed.\n","- Build an API using Flask or FastAPI to allow real-time predictions from the saved model."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/05_deployment.ipynb ----

---- START OF ./notebooks/04_visualizations.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CO2 Emissions Visualizations using Plotly\n","In this notebook, we will create visualizations to explore the trends and insights from the CO2 emissions data using Plotly."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import plotly.express as px\n","import plotly.graph_objects as go"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Preprocessed Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the preprocessed data\n","data_path = 'data/cleaned/co2_emissions_cleaned.csv'\n","df = pd.read_csv(data_path)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 1: Global CO2 Emissions Over Time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize total CO2 emissions for all countries over time\n","years = df.columns[2:]\n","global_emissions = df[years].sum()\n","\n","fig = px.line(x=years, y=global_emissions, labels={'x': 'Year', 'y': 'Global CO2 Emissions (kt)'},\n","              title='Global CO2 Emissions Over Time')\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 2: CO2 Emissions by Country (Choropleth Map)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create an interactive Plotly choropleth map of CO2 emissions by country for a specific year\n","df_2020 = df[['Country Name', '2020']]  # Assuming you want to visualize data for 2020\n","\n","fig = px.choropleth(df_2020, locations='Country Name', locationmode='country names',\n","                    color='2020', hover_name='Country Name',\n","                    title='CO2 Emissions by Country (2020)',\n","                    color_continuous_scale=px.colors.sequential.Plasma)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 3: Top 10 CO2 Emitting Countries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bar plot for top 10 CO2 emitting countries in 2020\n","top_10_countries = df[['Country Name', '2020']].sort_values(by='2020', ascending=False).head(10)\n","\n","fig = px.bar(top_10_countries, x='Country Name', y='2020',\n","             title='Top 10 CO2 Emitting Countries (2020)',\n","             labels={'2020': 'CO2 Emissions (kt)', 'Country Name': 'Country'})\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 4: CO2 Emissions Over Time for Selected Countries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Line plot for CO2 emissions over time for a few selected countries\n","selected_countries = ['United States', 'China', 'India', 'Russia', 'Germany']\n","df_selected = df[df['Country Name'].isin(selected_countries)]\n","df_selected = df_selected.melt(id_vars=['Country Name'], value_vars=years, \n","                               var_name='Year', value_name='CO2 Emissions (kt)')\n","\n","fig = px.line(df_selected, x='Year', y='CO2 Emissions (kt)', color='Country Name',\n","              title='CO2 Emissions Over Time for Selected Countries')\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 5: CO2 Emissions vs Population (Optional if Population Data is Available)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example scatter plot if you have population data\n","# Assuming df['Population'] contains population data for 2020\n","# fig = px.scatter(df, x='Population', y='2020', size='2020', hover_name='Country Name',\n","#                  title='CO2 Emissions vs Population (2020)',\n","#                  labels={'2020': 'CO2 Emissions (kt)', 'Population': 'Population'})\n","# fig.show()\n","# Customize further based on available data"]},{"cell_type":"markdown","metadata":{},"source":["## Next Steps\n","- Continue exploring the data using more Plotly visualizations.\n","- Consider using interactive elements (e.g., sliders or dropdowns) to allow users to filter the data dynamically."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/04_visualizations.ipynb ----

---- START OF ./notebooks/02_data_preprocessing.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CO2 Emission Data Preprocessing\n","In this notebook, we will preprocess the CO2 emissions dataset to prepare it for machine learning modeling. This involves handling missing data, scaling the features, and splitting the data for training and testing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Cleaned Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the cleaned dataset\n","data_path = 'data/cleaned/co2_emissions_cleaned.csv'\n","df = pd.read_csv(data_path)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Handle Missing Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fill or drop missing values\n","df.fillna(method='ffill', inplace=True)  # Forward fill to handle missing values\n","# You can also choose to drop rows or columns with too many missing values if necessary\n","# df.dropna(inplace=True)\n","df.isnull().sum()  # Check for remaining missing values"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Scale the features using StandardScaler\n","scaler = StandardScaler()\n","scaled_features = scaler.fit_transform(df.iloc[:, 4:])  # Scale the year columns (numeric)\n","scaled_df = pd.DataFrame(scaled_features, columns=df.columns[4:])\n","scaled_df.insert(0, 'Country Name', df['Country Name'])  # Add back the non-scaled columns\n","scaled_df.insert(1, 'Country Code', df['Country Code'])\n","scaled_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Splitting the Data for Training and Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define X (features) and y (target) for ML\n","X = scaled_df.iloc[:, 2:]  # All numeric columns (scaled)\n","y = df.iloc[:, 3]  # Example: You may want to predict emissions for the latest year available\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Next Steps\n","- After preprocessing, the data is ready for modeling.\n","- You can now use this preprocessed data in your machine learning models (e.g., regression, time-series forecasting)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/02_data_preprocessing.ipynb ----

---- START OF ./notebooks/03_model_training.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CO2 Emission ML Model Training and Evaluation\n","In this notebook, we will train a machine learning model to predict CO2 emissions based on the preprocessed data and evaluate the model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Preprocessed Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the preprocessed data\n","data_path = 'data/cleaned/co2_emissions_cleaned.csv'\n","df = pd.read_csv(data_path)\n","X = df.iloc[:, 2:]  # Feature columns\n","y = df.iloc[:, 1]   # Target column (CO2 emissions for a specific year)\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train.shape, X_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Train a Linear Regression Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the Linear Regression model\n","lr_model = LinearRegression()\n","lr_model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred_lr = lr_model.predict(X_test)\n","\n","# Evaluate the model\n","mse_lr = mean_squared_error(y_test, y_pred_lr)\n","r2_lr = r2_score(y_test, y_pred_lr)\n","print(f\"Linear Regression MSE: {mse_lr}\")\n","print(f\"Linear Regression R^2 Score: {r2_lr}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Train a Random Forest Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the Random Forest Regressor\n","rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred_rf = rf_model.predict(X_test)\n","\n","# Evaluate the model\n","mse_rf = mean_squared_error(y_test, y_pred_rf)\n","r2_rf = r2_score(y_test, y_pred_rf)\n","print(f\"Random Forest MSE: {mse_rf}\")\n","print(f\"Random Forest R^2 Score: {r2_rf}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Compare Model Performance\n","- Linear Regression Mean Squared Error vs Random Forest Mean Squared Error\n","- R^2 Score for both models"]},{"cell_type":"markdown","metadata":{},"source":["## Next Steps\n","- Based on the performance, choose the best model and perform further tuning if necessary.\n","- Save the model for deployment or further analysis."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/03_model_training.ipynb ----

