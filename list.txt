---- START OF ./app/__init__.py ----

from flask import Flask, render_template

# Initialize the Flask app
app = Flask(__name__)

# Define a route for the home page
@app.route('/')
def index():
    return render_template('index.html')

# Start the Flask app
if __name__ == '__main__':
    app.run(debug=True)
---- END OF ./app/__init__.py ----

---- START OF ./app/api.py ----
from flask import Flask, request, jsonify
import pandas as pd
import pickle

app = Flask(__name__)

# Load the trained model
model_path = 'models/co2_model.pkl'  # Path to your trained model

def load_model(model_path):
    """ Load the trained model from a file. """
    try:
        with open(model_path, 'rb') as file:
            model = pickle.load(file)
        print(f"Model loaded successfully from {model_path}")
        return model
    except Exception as e:
        print(f"Error loading the model: {e}")
        return None

# Load the model
model = load_model(model_path)

# API endpoint to make predictions
@app.route('/api/predict', methods=['POST'])
def predict():
    try:
        # Validate if the model is loaded
        if model is None:
            return jsonify({'error': 'Model is not available'}), 500
        
        # Get input data from the request
        input_data = request.get_json()

        if not input_data:
            return jsonify({'error': 'No input data provided'}), 400
        
        input_df = pd.DataFrame(input_data)

        # Check if input data has any missing values
        if input_df.isnull().values.any():
            return jsonify({'error': 'Input data contains missing values'}), 400
        
        # Make predictions using the loaded model
        predictions = model.predict(input_df)

        return jsonify({'predictions': predictions.tolist()})
    
    except Exception as e:
        return jsonify({'error': f"An error occurred: {str(e)}"}), 500

# API endpoint for health check
@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'API is running'}), 200

if __name__ == '__main__':
    app.run(debug=True)
---- END OF ./app/api.py ----

---- START OF ./app/routes.py ----
from flask import Flask, render_template, jsonify, request
import pandas as pd
import pickle

app = Flask(__name__)

# Route for the home page
@app.route('/')
def index():
    return render_template('index.html')

# Route for visualizations or data summary
@app.route('/summary')
def summary():
    # Path to the cleaned CO2 data
    data_path = 'data/cleaned/co2_emissions_cleaned.csv'  # Replace with actual path
    
    try:
        # Load the dataset
        df = pd.read_csv(data_path)
        
        # Check if the dataset is empty
        if df.empty:
            return jsonify({'error': 'Dataset is empty'}), 404
        
        # Summarize the data
        summary_data = df.describe().to_dict()
        return jsonify(summary_data)
    
    except FileNotFoundError:
        return jsonify({'error': 'Data file not found'}), 404
    except Exception as e:
        return jsonify({'error': f"An error occurred: {str(e)}"}), 500

# Route to predict CO2 emissions (if using a model)
@app.route('/predict', methods=['POST'])
def predict():
    model_path = 'models/co2_model.pkl'  # Path to your trained model
    
    try:
        # Load the trained model
        with open(model_path, 'rb') as file:
            model = pickle.load(file)
        
        # Get the input data from the POST request
        input_data = request.get_json()
        
        if not input_data:
            return jsonify({'error': 'No input data provided'}), 400
        
        input_df = pd.DataFrame(input_data)
        
        # Check for missing values in the input data
        if input_df.isnull().values.any():
            return jsonify({'error': 'Input data contains missing values'}), 400
        
        # Make predictions using the loaded model
        predictions = model.predict(input_df)
        
        return jsonify({'predictions': predictions.tolist()})
    
    except FileNotFoundError:
        return jsonify({'error': 'Model file not found'}), 500
    except Exception as e:
        return jsonify({'error': f"An error occurred: {str(e)}"}), 500

if __name__ == '__main__':
    app.run(debug=True)
---- END OF ./app/routes.py ----

---- START OF ./requirements.txt ----

Flask==2.0.1
pandas==1.3.3
scikit-learn==0.24.2
plotly==5.3.1
numpy==1.21.2
gunicorn==20.1.0
sqlite3==3.35.4
---- END OF ./requirements.txt ----

---- START OF ./config/settings.json ----
{
    "database": {
        "type": "sqlite",
        "url": "sqlite:///emissions_data.db"
    },
    "api": {
        "prediction_endpoint": "/api/predict",
        "health_endpoint": "/api/health"
    },
    "api_keys": {
        "service_1": "your_service_1_api_key_here",
        "service_2": "your_service_2_api_key_here"
    }
}---- END OF ./config/settings.json ----

---- START OF ./config/runtime.txt ----

python-3.8.12
---- END OF ./config/runtime.txt ----

---- START OF ./tests/test_model.py ----

import unittest
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import pickle

class TestModelTraining(unittest.TestCase):
    
    def setUp(self):
        # Example training data
        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
        y = np.array([2, 3, 4, 5, 6])
        
        # Split the data into training and test sets
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Train a simple Linear Regression model
        self.model = LinearRegression()
        self.model.fit(self.X_train, self.y_train)
        
        # Save the model using pickle (mocking real use case)
        self.model_file = 'test_model.pkl'
        with open(self.model_file, 'wb') as file:
            pickle.dump(self.model, file)
    
    def test_model_training(self):
        # Check if the model was trained successfully
        y_pred_train = self.model.predict(self.X_train)
        self.assertEqual(len(y_pred_train), len(self.y_train), "Model training failed")
    
    def test_model_prediction(self):
        # Test model prediction accuracy on the test set
        y_pred_test = self.model.predict(self.X_test)
        mse = mean_squared_error(self.y_test, y_pred_test)
        self.assertLess(mse, 0.1, "Model test predictions are not accurate enough")
    
    def tearDown(self):
        # Clean up the saved model file
        import os
        if os.path.exists(self.model_file):
            os.remove(self.model_file)

if __name__ == '__main__':
    unittest.main()
---- END OF ./tests/test_model.py ----

---- START OF ./tests/test_data_cleaning.py ----

import unittest
import pandas as pd
from scripts.data_cleaning import clean_data

class TestDataCleaning(unittest.TestCase):
    
    def setUp(self):
        # Example raw data for testing
        data = {
            'Country Name': ['Country A', 'Country B', 'Country C'],
            'Year 1': [10, 20, None],
            'Year 2': [15, None, 30],
            'Year 3': [None, 25, 35]
        }
        self.df = pd.DataFrame(data)
    
    def test_clean_data(self):
        # Run the data cleaning function
        df_cleaned = clean_data(self.df)
        
        # Check if missing values are filled
        self.assertFalse(df_cleaned.isnull().values.any(), "There are still missing values in the cleaned data")
        
        # Check if the shape of the DataFrame remains unchanged
        self.assertEqual(df_cleaned.shape, self.df.shape, "DataFrame shape changed after cleaning")
    
    def tearDown(self):
        # Clean up (if necessary)
        pass

if __name__ == '__main__':
    unittest.main()
---- END OF ./tests/test_data_cleaning.py ----

---- START OF ./tests/test_api.py ----

import unittest
import json
from flask import Flask
from app.api import app  # Assuming 'api.py' is in the 'app' folder

class TestAPI(unittest.TestCase):
    
    def setUp(self):
        # Set up the test client for the Flask app
        self.app = app.test_client()
        self.app.testing = True

    def test_health_check(self):
        # Test the health check endpoint
        response = self.app.get('/api/health')
        self.assertEqual(response.status_code, 200)
        data = json.loads(response.data)
        self.assertEqual(data['status'], 'API is running')

    def test_prediction(self):
        # Test the prediction endpoint with mock input data
        mock_input = {
            "feature1": [6, 7, 8],
            "feature2": [7, 8, 9]
        }
        response = self.app.post('/api/predict', data=json.dumps(mock_input), content_type='application/json')
        self.assertEqual(response.status_code, 200)
        data = json.loads(response.data)
        self.assertIn('predictions', data)
        self.assertIsInstance(data['predictions'], list)

if __name__ == '__main__':
    unittest.main()
---- END OF ./tests/test_api.py ----

---- START OF ./models/co2_model.pkl ----
Äï¡      åsklearn.linear_model._baseîåLinearRegressionîìî)Åî}î(åfit_interceptîàå	normalizeîå
deprecatedîåcopy_Xîàån_jobsîNåpositiveîâån_features_in_îKåcoef_îånumpy.core.multiarrayîå_reconstructîìîånumpyîåndarrayîìîK ÖîCbîáîRî(KKÖîhådtypeîìîåf8îâàáîRî(Kå<îNNNJˇˇˇˇJˇˇˇˇK tîbâC      ‡?      ‡?îtîbårank_îKå	singular_îhhK ÖîháîRî(KKÖîhâC®Ùóõw„@ˇ†ã˘Ô\ª<îtîbå
intercept_îhåscalarîìîhC      ‡?îÜîRîå_sklearn_versionîå1.1.3îub.---- END OF ./models/co2_model.pkl ----

---- START OF ./static/css/style.css ----

/* Basic styles for the web interface */

body {
    font-family: Arial, sans-serif;
    background-color: #f4f4f9;
    color: #333;
    margin: 0;
    padding: 0;
}

header {
    background-color: #4CAF50;
    color: white;
    padding: 10px 0;
    text-align: center;
}

h1, h2 {
    color: #333;
}

.container {
    width: 80%;
    margin: 0 auto;
}

button {
    background-color: #4CAF50;
    color: white;
    padding: 10px 20px;
    border: none;
    cursor: pointer;
    border-radius: 4px;
    margin: 10px 0;
}

button:hover {
    background-color: #45a049;
}

footer {
    text-align: center;
    padding: 10px;
    background-color: #4CAF50;
    color: white;
    position: fixed;
    bottom: 0;
    width: 100%;
}

.chart-container {
    width: 100%;
    height: 400px;
    margin-top: 20px;
}
---- END OF ./static/css/style.css ----

---- START OF ./static/js/dashboard.js ----

// JavaScript for interactive visualizations using Plotly

document.addEventListener('DOMContentLoaded', function () {
    // Example data for CO2 emissions over time
    var years = ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020'];
    var emissions = [10000, 12000, 15000, 14000, 13000, 12500, 13500, 14500, 16000, 17000, 18000];  // Example values

    var trace1 = {
        x: years,
        y: emissions,
        type: 'scatter',
        mode: 'lines+markers',
        name: 'CO2 Emissions',
        line: { shape: 'linear', color: '#1f77b4' },
        marker: { size: 8 }
    };

    var data = [trace1];

    var layout = {
        title: 'CO2 Emissions Over Time',
        xaxis: { title: 'Year' },
        yaxis: { title: 'CO2 Emissions (kt)' }
    };

    // Render the chart in a div with id 'co2-chart'
    Plotly.newPlot('co2-chart', data, layout);
});
---- END OF ./static/js/dashboard.js ----

---- START OF ./scripts/train_model.py ----

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import pickle

def load_data(file_path):
    """ Load the cleaned dataset for model training. """
    try:
        df = pd.read_csv(file_path)
        print(f"Data loaded successfully from {file_path}")
        return df
    except FileNotFoundError as e:
        print(f"File not found: {e}")
        return None

def train_model(X_train, y_train):
    """ Train a Linear Regression model. """
    model = LinearRegression()
    model.fit(X_train, y_train)
    print("Model training completed.")
    return model

def evaluate_model(model, X_test, y_test):
    """ Evaluate the model's performance. """
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Mean Squared Error: {mse}")
    print(f"R^2 Score: {r2}")
    return mse, r2

def save_model(model, output_file_path):
    """ Save the trained model to a file using pickle. """
    with open(output_file_path, 'wb') as file:
        pickle.dump(model, file)
    print(f"Model saved to {output_file_path}")

if __name__ == "__main__":
    # File paths
    input_file_path = 'data/cleaned/co2_emissions_cleaned.csv'  # Replace with your actual path
    model_output_path = 'models/co2_model.pkl'  # Path to save the trained model
    
    # Load the cleaned data
    df = load_data(input_file_path)
    
    if df is not None:
        # Define features (X) and target (y)
        X = df.iloc[:, 2:]  # Example: features are columns from index 2 onwards
        y = df.iloc[:, 1]   # Example: target is the second column (CO2 emissions)

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train the model
        model = train_model(X_train, y_train)

        # Evaluate the model
        evaluate_model(model, X_test, y_test)

        # Save the trained model
        save_model(model, model_output_path)
---- END OF ./scripts/train_model.py ----

---- START OF ./scripts/predict.py ----

import pickle
import pandas as pd

def load_model(model_file_path):
    """ Load the trained model from a file. """
    try:
        with open(model_file_path, 'rb') as file:
            model = pickle.load(file)
        print(f"Model loaded successfully from {model_file_path}")
        return model
    except FileNotFoundError as e:
        print(f"Model file not found: {e}")
        return None

def make_prediction(model, input_data):
    """ Make predictions using the loaded model and input data. """
    predictions = model.predict(input_data)
    return predictions

if __name__ == "__main__":
    # Path to the saved model
    model_file_path = 'models/co2_model.pkl'  # Replace with the actual model file path

    # Load the model
    model = load_model(model_file_path)

    if model is not None:
        # Example input data (replace with actual data or dynamically load it)
        input_data = pd.DataFrame({
            'feature1': [6, 7, 8],  # Example feature columns
            'feature2': [7, 8, 9]
        })

        # Make predictions
        predictions = make_prediction(model, input_data)

        # Display the predictions
        print("Predictions for the input data:")
        print(predictions)
---- END OF ./scripts/predict.py ----

---- START OF ./scripts/database_setup.py ----

import sqlite3

def create_database(db_name):
    """ Create a SQLite database and connect to it. """
    conn = sqlite3.connect(db_name)
    print(f"Database {db_name} created and connected successfully.")
    return conn

def create_table(conn):
    """ Create a table for storing CO2 emissions data. """
    create_table_sql = '''
    CREATE TABLE IF NOT EXISTS emissions_data (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        country_name TEXT NOT NULL,
        country_code TEXT,
        indicator_name TEXT,
        indicator_code TEXT,
        year_1960 REAL,
        year_1961 REAL,
        year_1962 REAL,
        year_1963 REAL,
        year_1964 REAL,
        year_1965 REAL,
        year_1966 REAL,
        year_1967 REAL,
        year_1968 REAL,
        year_1969 REAL,
        year_1970 REAL,
        year_1971 REAL,
        year_1972 REAL,
        year_1973 REAL,
        year_1974 REAL,
        year_1975 REAL,
        year_1976 REAL,
        year_1977 REAL,
        year_1978 REAL,
        year_1979 REAL,
        year_1980 REAL,
        year_1981 REAL,
        year_1982 REAL,
        year_1983 REAL,
        year_1984 REAL,
        year_1985 REAL,
        year_1986 REAL,
        year_1987 REAL,
        year_1988 REAL,
        year_1989 REAL,
        year_1990 REAL,
        year_1991 REAL,
        year_1992 REAL,
        year_1993 REAL,
        year_1994 REAL,
        year_1995 REAL,
        year_1996 REAL,
        year_1997 REAL,
        year_1998 REAL,
        year_1999 REAL,
        year_2000 REAL,
        year_2001 REAL,
        year_2002 REAL,
        year_2003 REAL,
        year_2004 REAL,
        year_2005 REAL,
        year_2006 REAL,
        year_2007 REAL,
        year_2008 REAL,
        year_2009 REAL,
        year_2010 REAL,
        year_2011 REAL,
        year_2012 REAL,
        year_2013 REAL,
        year_2014 REAL,
        year_2015 REAL,
        year_2016 REAL,
        year_2017 REAL,
        year_2018 REAL,
        year_2019 REAL,
        year_2020 REAL
    );
    '''
    conn.execute(create_table_sql)
    print("Table 'emissions_data' created successfully.")

def close_connection(conn):
    """ Close the connection to the database. """
    conn.close()
    print("Database connection closed.")

if __name__ == "__main__":
    # Database name
    db_name = 'emissions_data.db'

    # Create the database and table
    conn = create_database(db_name)
    create_table(conn)

    # Close the connection
    close_connection(conn)
---- END OF ./scripts/database_setup.py ----

---- START OF ./scripts/data_cleaning.py ----

import pandas as pd

def load_data(file_path):
    """ Load the CO2 emissions dataset. """
    try:
        df = pd.read_csv(file_path, skiprows=4)  # Adjust for file structure (skip metadata rows)
        print(f"Data loaded successfully from {file_path}")
        return df
    except FileNotFoundError as e:
        print(f"File not found: {e}")
        return None

def clean_data(df):
    """ Clean and preprocess the CO2 emissions data. """
    # Remove unnamed columns
    df_cleaned = df.loc[:, ~df.columns.str.contains('^Unnamed')]
    
    # Fill missing values with forward fill method
    df_cleaned.fillna(method='ffill', inplace=True)
    
    # Drop rows with all NaN values (in case of incomplete rows)
    df_cleaned.dropna(how='all', inplace=True)
    
    print("Data cleaning completed.")
    return df_cleaned

def save_cleaned_data(df, output_file_path):
    """ Save the cleaned data to a new CSV file. """
    df.to_csv(output_file_path, index=False)
    print(f"Cleaned data saved to {output_file_path}")

if __name__ == "__main__":
    # File paths
    input_file_path = 'data/raw/co2_emissions.csv'  # Replace with your actual path
    output_file_path = 'data/cleaned/co2_emissions_cleaned.csv'  # Replace with your actual path
    
    # Load raw data
    df = load_data(input_file_path)
    
    # Clean and preprocess the data
    if df is not None:
        df_cleaned = clean_data(df)
        
        # Save the cleaned data
        save_cleaned_data(df_cleaned, output_file_path)
---- END OF ./scripts/data_cleaning.py ----

---- START OF ./templates/index.html ----
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CO2 Emission Dashboard</title>
    <link rel="stylesheet" href="/static/css/style.css">
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="/static/js/dashboard.js" defer></script>
</head>

<body>
    <header>
        <h1>CO2 Emission Dashboard</h1>
    </header>

    <div class="container">
        <h2>CO2 Emissions Over Time</h2>
        <div id="co2-chart" class="chart-container"></div>
    </div>

    <footer>
        <p>&copy; 2024 CO2 Emission Dashboard</p>
    </footer>
</body>

</html>---- END OF ./templates/index.html ----

---- START OF ./data/database/emissions_data.sql ----
CREATE TABLE
    emissions_data (
        id SERIAL PRIMARY KEY,
        country_name VARCHAR(255),
        country_code VARCHAR(10),
        indicator_name VARCHAR(255),
        indicator_code VARCHAR(50),
        year_1960 FLOAT,
        year_1961 FLOAT,
        year_1962 FLOAT,
        year_1963 FLOAT,
        year_1964 FLOAT,
        year_1965 FLOAT,
        year_1966 FLOAT,
        year_1967 FLOAT,
        year_1968 FLOAT,
        year_1969 FLOAT,
        year_1970 FLOAT,
        year_1971 FLOAT,
        year_1972 FLOAT,
        year_1973 FLOAT,
        year_1974 FLOAT,
        year_1975 FLOAT,
        year_1976 FLOAT,
        year_1977 FLOAT,
        year_1978 FLOAT,
        year_1979 FLOAT,
        year_1980 FLOAT,
        year_1981 FLOAT,
        year_1982 FLOAT,
        year_1983 FLOAT,
        year_1984 FLOAT,
        year_1985 FLOAT,
        year_1986 FLOAT,
        year_1987 FLOAT,
        year_1988 FLOAT,
        year_1989 FLOAT,
        year_1990 FLOAT,
        year_1991 FLOAT,
        year_1992 FLOAT,
        year_1993 FLOAT,
        year_1994 FLOAT,
        year_1995 FLOAT,
        year_1996 FLOAT,
        year_1997 FLOAT,
        year_1998 FLOAT,
        year_1999 FLOAT,
        year_2000 FLOAT,
        year_2001 FLOAT,
        year_2002 FLOAT,
        year_2003 FLOAT,
        year_2004 FLOAT,
        year_2005 FLOAT,
        year_2006 FLOAT,
        year_2007 FLOAT,
        year_2008 FLOAT,
        year_2009 FLOAT,
        year_2010 FLOAT,
        year_2011 FLOAT,
        year_2012 FLOAT,
        year_2013 FLOAT,
        year_2014 FLOAT,
        year_2015 FLOAT,
        year_2016 FLOAT,
        year_2017 FLOAT,
        year_2018 FLOAT,
        year_2019 FLOAT,
        year_2020 FLOAT,
        year_2021 FLOAT,
        year_2022 FLOAT,
        year_2023 FLOAT
    );---- END OF ./data/database/emissions_data.sql ----

---- START OF ./notebooks/01_data_exploration.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# CO2 Emission Data Exploration\n","In this notebook, we will explore the CO2 emissions dataset to get a better understanding of its structure, missing values, and general trends. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the dataset\n","data_path = 'data/raw/co2_emissions.csv'\n","df = pd.read_csv(data_path, skiprows=4)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Check for Missing Values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check for missing values\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["## Summary Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display summary statistics of the dataset\n","df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualize Trends Over Time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example visualization of CO2 emissions for a country over time\n","df_country = df[df['Country Name'] == 'United States']\n","years = df.columns[4:-1]\n","emissions = df_country[years].T\n","plt.figure(figsize=(10,6))\n","plt.plot(years, emissions)\n","plt.title('CO2 Emissions Over Time (United States)')\n","plt.xlabel('Year')\n","plt.ylabel('CO2 Emissions (kt)')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Further Exploration Ideas\n","- Investigate emissions for other countries.\n","- Analyze the countries with the highest and lowest emissions.\n","- Check if there's a trend in global emissions over time."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/01_data_exploration.ipynb ----

---- START OF ./notebooks/05_deployment.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CO2 Emission Model Deployment and Performance Testing\n","In this notebook, we will save the trained machine learning model and test its performance in a deployment-like environment."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pickle\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.linear_model import LinearRegression  # Example ML model"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Preprocessed Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the preprocessed data\n","data_path = 'data/cleaned/co2_emissions_cleaned.csv'\n","df = pd.read_csv(data_path)\n","X = df.iloc[:, 2:]  # Feature columns\n","y = df.iloc[:, 1]   # Target column (CO2 emissions for a specific year)\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train and Save the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train a Linear Regression model (as an example)\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Save the trained model using pickle\n","model_filename = 'models/co2_model.pkl'\n","with open(model_filename, 'wb') as file:\n","    pickle.dump(model, file)\n","print(f'Model saved as {model_filename}')"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Model and Test Performance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the saved model\n","with open(model_filename, 'rb') as file:\n","    loaded_model = pickle.load(file)\n","print('Model loaded successfully')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate Model Performance on Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions on the test set\n","y_pred = loaded_model.predict(X_test)\n","\n","# Evaluate performance\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","print(f'Mean Squared Error: {mse}')\n","print(f'R^2 Score: {r2}')"]},{"cell_type":"markdown","metadata":{},"source":["## Next Steps\n","- Deploy the model to a cloud environment (e.g., AWS, Google Cloud) if needed.\n","- Build an API using Flask or FastAPI to allow real-time predictions from the saved model."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/05_deployment.ipynb ----

---- START OF ./notebooks/04_visualizations.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CO2 Emissions Visualizations using Plotly\n","In this notebook, we will create visualizations to explore the trends and insights from the CO2 emissions data using Plotly."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import plotly.express as px\n","import plotly.graph_objects as go"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Preprocessed Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the preprocessed data\n","data_path = 'data/cleaned/co2_emissions_cleaned.csv'\n","df = pd.read_csv(data_path)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 1: Global CO2 Emissions Over Time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize total CO2 emissions for all countries over time\n","years = df.columns[2:]\n","global_emissions = df[years].sum()\n","\n","fig = px.line(x=years, y=global_emissions, labels={'x': 'Year', 'y': 'Global CO2 Emissions (kt)'},\n","              title='Global CO2 Emissions Over Time')\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 2: CO2 Emissions by Country (Choropleth Map)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create an interactive Plotly choropleth map of CO2 emissions by country for a specific year\n","df_2020 = df[['Country Name', '2020']]  # Assuming you want to visualize data for 2020\n","\n","fig = px.choropleth(df_2020, locations='Country Name', locationmode='country names',\n","                    color='2020', hover_name='Country Name',\n","                    title='CO2 Emissions by Country (2020)',\n","                    color_continuous_scale=px.colors.sequential.Plasma)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 3: Top 10 CO2 Emitting Countries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bar plot for top 10 CO2 emitting countries in 2020\n","top_10_countries = df[['Country Name', '2020']].sort_values(by='2020', ascending=False).head(10)\n","\n","fig = px.bar(top_10_countries, x='Country Name', y='2020',\n","             title='Top 10 CO2 Emitting Countries (2020)',\n","             labels={'2020': 'CO2 Emissions (kt)', 'Country Name': 'Country'})\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 4: CO2 Emissions Over Time for Selected Countries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Line plot for CO2 emissions over time for a few selected countries\n","selected_countries = ['United States', 'China', 'India', 'Russia', 'Germany']\n","df_selected = df[df['Country Name'].isin(selected_countries)]\n","df_selected = df_selected.melt(id_vars=['Country Name'], value_vars=years, \n","                               var_name='Year', value_name='CO2 Emissions (kt)')\n","\n","fig = px.line(df_selected, x='Year', y='CO2 Emissions (kt)', color='Country Name',\n","              title='CO2 Emissions Over Time for Selected Countries')\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Visualization 5: CO2 Emissions vs Population (Optional if Population Data is Available)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example scatter plot if you have population data\n","# Assuming df['Population'] contains population data for 2020\n","# fig = px.scatter(df, x='Population', y='2020', size='2020', hover_name='Country Name',\n","#                  title='CO2 Emissions vs Population (2020)',\n","#                  labels={'2020': 'CO2 Emissions (kt)', 'Population': 'Population'})\n","# fig.show()\n","# Customize further based on available data"]},{"cell_type":"markdown","metadata":{},"source":["## Next Steps\n","- Continue exploring the data using more Plotly visualizations.\n","- Consider using interactive elements (e.g., sliders or dropdowns) to allow users to filter the data dynamically."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/04_visualizations.ipynb ----

---- START OF ./notebooks/02_data_preprocessing.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CO2 Emission Data Preprocessing\n","In this notebook, we will preprocess the CO2 emissions dataset to prepare it for machine learning modeling. This involves handling missing data, scaling the features, and splitting the data for training and testing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Cleaned Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the cleaned dataset\n","data_path = 'data/cleaned/co2_emissions_cleaned.csv'\n","df = pd.read_csv(data_path)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Handle Missing Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fill or drop missing values\n","df.fillna(method='ffill', inplace=True)  # Forward fill to handle missing values\n","# You can also choose to drop rows or columns with too many missing values if necessary\n","# df.dropna(inplace=True)\n","df.isnull().sum()  # Check for remaining missing values"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Scale the features using StandardScaler\n","scaler = StandardScaler()\n","scaled_features = scaler.fit_transform(df.iloc[:, 4:])  # Scale the year columns (numeric)\n","scaled_df = pd.DataFrame(scaled_features, columns=df.columns[4:])\n","scaled_df.insert(0, 'Country Name', df['Country Name'])  # Add back the non-scaled columns\n","scaled_df.insert(1, 'Country Code', df['Country Code'])\n","scaled_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Splitting the Data for Training and Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define X (features) and y (target) for ML\n","X = scaled_df.iloc[:, 2:]  # All numeric columns (scaled)\n","y = df.iloc[:, 3]  # Example: You may want to predict emissions for the latest year available\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Next Steps\n","- After preprocessing, the data is ready for modeling.\n","- You can now use this preprocessed data in your machine learning models (e.g., regression, time-series forecasting)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/02_data_preprocessing.ipynb ----

---- START OF ./notebooks/03_model_training.ipynb ----
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CO2 Emission ML Model Training and Evaluation\n","In this notebook, we will train a machine learning model to predict CO2 emissions based on the preprocessed data and evaluate the model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"markdown","metadata":{},"source":["## Load the Preprocessed Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the preprocessed data\n","data_path = 'data/cleaned/co2_emissions_cleaned.csv'\n","df = pd.read_csv(data_path)\n","X = df.iloc[:, 2:]  # Feature columns\n","y = df.iloc[:, 1]   # Target column (CO2 emissions for a specific year)\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train.shape, X_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Train a Linear Regression Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the Linear Regression model\n","lr_model = LinearRegression()\n","lr_model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred_lr = lr_model.predict(X_test)\n","\n","# Evaluate the model\n","mse_lr = mean_squared_error(y_test, y_pred_lr)\n","r2_lr = r2_score(y_test, y_pred_lr)\n","print(f\"Linear Regression MSE: {mse_lr}\")\n","print(f\"Linear Regression R^2 Score: {r2_lr}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Train a Random Forest Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the Random Forest Regressor\n","rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred_rf = rf_model.predict(X_test)\n","\n","# Evaluate the model\n","mse_rf = mean_squared_error(y_test, y_pred_rf)\n","r2_rf = r2_score(y_test, y_pred_rf)\n","print(f\"Random Forest MSE: {mse_rf}\")\n","print(f\"Random Forest R^2 Score: {r2_rf}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Compare Model Performance\n","- Linear Regression Mean Squared Error vs Random Forest Mean Squared Error\n","- R^2 Score for both models"]},{"cell_type":"markdown","metadata":{},"source":["## Next Steps\n","- Based on the performance, choose the best model and perform further tuning if necessary.\n","- Save the model for deployment or further analysis."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
---- END OF ./notebooks/03_model_training.ipynb ----

